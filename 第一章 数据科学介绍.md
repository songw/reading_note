#data-science/tutorial/chapter1  
[https://www.oracle.com/cn/data-science/what-is-data-science/](https://www.oracle.com/cn/data-science/what-is-data-science/)
# 什么是数据科学？
数据科学包括一套原则、问题定义、算法和流程， 用于从大型数据集中提取不明显和有用的模式。 数据科学的许多要素已经在机器学习和数据挖掘等相关领域得到了发展。事实上，数据科学、机器学习和数据挖掘这三个术语经常可以互换使用。这些学科的共同点是专注于通过数据分析来改进决策制定。然而，尽管数据科学借鉴了这些领域，但它的范围更广。机器学习(ML)专注于从数据中提取模式的算法的设计和评估。数据挖掘通常处理结构化数据的分析，并且通常意味着强调商业应用。数据科学考虑了所有这些因素， 但也应对了其他挑战。例如非结构化社交媒体和Web数据的捕获、清理和转换;使用大数据技术存储和处理大型非结构化数据集;以及与数据道德和监管有关的问题。

利用数据科学，我们可以提取不同类型的模式。 例如，我们可能希望提取模式，以帮助我们识别表现出相似行为和品味的客户群。在商业术语中， 这项任务被称为客户细分，而在数据科学术语中， 它被称为聚类。或者，我们可能希望提取一个模式来识别经常一起购买的产品，这一过程称为关联规则挖掘。或者，我们可能想要提取识别奇怪或异常事件的模式，例如欺诈性保险索赔，这一过程称为异常或异常值检测。最后，我们可能想要识别帮助我们对事物进行分类的模式。例如， 以下规则说明了从电子邮件数据集中提取的分类模式可能是什么样子:如果电子邮件包含短语“轻 松赚钱”，则很可能是垃圾邮件。识别这些类型的分类规则称为预测。“预测”这个词似乎是一个奇怪的选择，因为这个规则并不能预测未来会发生什么:电子邮件已经是或不是垃圾邮件。因此， 最好将预测模式视为预测属性的缺失值，而不是预测未来。在此示例中，我们预测电子邮件分类属性的值是否应为“垃圾邮件”。

尽管我们可以使用数据科学来提取不同类型的模式，但我们总是希望这些模式既不明显又有用。上一段中给出的示例电子邮件分类规则非常简单明了，如果它是数据科学过程中提取的唯一规则，我们会感到失望。例如，此电子邮件分类规则仅检查电子邮件的一个属性：该电子邮件是否包含短语“轻松赚钱”？如果人类专家可以轻松地在他或她自己的脑海中创建一个模式，那么通常不值得花费时间和精力使用数据科学来“发现”它。一般来说，当我们有大量数据示例并且模式过于复杂以至于人类无法手动发现和提取时，数据科学就会变得有用。作为下界，我们可以将大量数据示例定义为人类专家无法轻松检查的范围。关于模式的复杂性，我们可以再次根据人类的能力来定义它。我们人类相当擅长定义检查一个、两个甚至三个属性（通常也称为特征或变量）的规则，但是当我们超过三个属性时，我们可能开始难以处理它们之间的相互作用。相比之下，数据科学适用于从成百上千，甚至在极端情况下，从数百万个属性中提取模式。

我们使用数据科学提取的模式只有能让我们深入了解问题并帮助我们解决问题时才有用。可行动的洞察力通常用于描述我们希望提取的模式给我们什么。术语洞察力强调从数据中提取的模式能够给到我们关于问题的不明显的相关信息。术语可行动强调我们有能力将从数据中获取的洞察付诸行动。例如，假设我们正在为一家试图解决客户流失问题的手机公司工作——也就是说，太多的客户正在转向其他公司。可以使用数据科学来解决这个问题的一种方法是从以前流失客户的数据中提取模式，这使我们能够识别当前存在流失风险的客户，然后联系这些客户并试图说服他们留在我们身边。只有在以下情况下，识别可能流失客户的模式才对我们有用：(a) 模式足够早地识别出客户，我们有足够的时间在他们流失之前与他们联系，并且 (b) 我们公司能够组建团队联系他们。为了使公司能够根据模式给予我们的洞察力采取行动，这两件事都是必需的。

# 数据收集的历史
最早的记录数据的方法可能是在棍子上刻上凹口来标记日子的过去，或者用杆子插在地上来标记冬至或夏至的日出。然而，随着写作的发展，我们记录自己的经历和世界事件的能力大大增加了我们收集的数据量。最早的文字形式在公元前 3200 年左右在美索不达米亚发展起来，用于保存商业记录。这种类型的记录保存的是商品的交易数据。交易数据包括诸如物品销售、发票开具、货物交付、信用卡支付、保险索赔等事件信息。非交易数据，例如人口统计数据，也有很长的历史。已知最早的人口普查发生在公元前 3000 年左右的法老时代埃及。早期各州在大型数据收集行动中投入如此多精力和资源的原因是这些州需要增加税收和军队，从而证明了本杰明富兰克林的说法，即生命中只有两件事是确定的：死亡和税收。

在过去的 150 年里，电子传感器的发展、数据的数字化以及计算机的发明使得收集和存储的数据量大幅增加。数据收集和存储的里程碑发生在 1970 年，当时 Edgar F. Codd 发表了一篇解释关系型数据模型的论文，该模型在确定如何存储、索引和从数据库中检索数据方面具有革命性意义。关系型数据模型使用户能够使用简单的查询从数据库中提取数据，这些查询定义了用户想要的数据，而无需用户担心数据的底层结构或它们的物理存储位置。Codd 的论文为现代数据库和结构化查询语言 (SQL) 的发展奠定了基础，SQL 是定义数据库查询的国际标准。关系数据库将数据存储在表中，表的结构为每个实例一行，每个属性一列。这种结构非常适合存储数据，因为它可以分解为自然属性。

数据库是用于存储和检索结构化交易或运营数据（即公司日常运营产生的数据）的常用技术。然而，随着公司规模越来越大、自动化程度越来越高，这些公司不同部门产生的数据量和种类都在急剧增加。在 1990 年代，公司意识到虽然他们积累了大量数据，但在分析这些数据时却一再遇到困难。部分问题在于数据通常存储在一个组织内的多个独立数据库中。另一个困难是数据库针对数据的存储和检索进行了优化，这些活动的特点是大量简单操作，例如 SELECT、INSERT、UPDATE 和 DELETE。为了分析他们的数据，这些公司需要能够汇集和协调来自不同数据库的数据并促进更复杂的分析数据操作的技术。这一业务挑战促成了数据仓库的发展。在数据仓库中，从整个组织中获取数据并进行集成，从而为分析提供更全面的数据集。

在过去的几十年里，我们的设备已经变得移动化和网络化，我们中的许多人现在每天在线上花费大量时间。技术和生活方式的转变对收集的数据量产生了巨大影响。据估计，自写作发明到 2003 年的五千年中收集的数据量约为 5 艾字节。自 2013 年以来，人类每天都会生成和存储相同数量的数据。然而，不仅收集的数据量急剧增长，而且数据的种类也急剧增加。只需考虑以下在线数据源列表：电子邮件、博客、照片、推文、喜欢、分享、网络搜索、视频上传、在线购买、播客，我们就可以知道数据的种类是如此至多。如果我们再考虑这些事件的元数据（描述原始数据的结构和属性的数据），我们就可以开始理解大数据这个术语的含义。大数据通常根据三个 V 来定义：极大数据量、数据类型的多样性以及必须处理数据的速度。

大数据的出现推动了一系列新的数据库技术的发展。这种新一代数据库通常被称为“NoSQL 数据库”。它们通常具有比传统关系数据库更简单的数据模型。 NoSQL 数据库使用对象表示法语言（例如 JavaScript 对象表示法 (JSON)）将数据存储为具有属性的对象。使用对象表示数据（与基于关系表的模型相比）的优点是每个对象的属性集都封装在对象中，具有很大的灵活性。例如，与其他对象相比，数据库中的一个对象可能只有属性的一个子集。相比之下，在关系数据库使用的标准表格数据结构中，所有数据点都应该具有相同的属性集（即列）。在数据不能（由于多样性或类型）自然地分解为一组结构化属性的情况下，对象表示的这种灵活性很重要。例如，很难定义用于表示自由文本（例如推文）或图像的属性集。然而，尽管这种表示灵活性允许我们以各种格式捕获和存储数据，但是首先必须对这些数据格式化，然后才能对其进行分析。

大数据的存在也促进了新的数据处理框架的发展。当需要以较高的速度处理大量数据时，从计算和速度的角度来看，将数据分布在多个服务器上，每个服务器并行地对数据进行处理，然后合并这些数据是很有用的。这是 MapReduce 框架在 Hadoop 上采用的方法。在 MapReduce 框架中，数据和查询被映射到（或分布在）多个服务器上，然后在每个服务器上计算的部分结果被归约（合并）在一起。

# 数据分析的历史
统计学是处理数据收集和分析的科学分支。统计一词最初是指收集和分析有关国家的数据，例如人口统计数据或经济数据。然而，随着时间的推移，应用统计分析的数据类型不断扩大，以至于今天的统计数据被用于分析所有类型的数据。数据统计分析的最简单形式是根据汇总（描述性）统计（包括集中趋势的度量，例如算术平均值，或离散趋势的度量，例如范围）。然而，在 17 世纪和 18 世纪，卡尔达诺（Gerolamo Cardano）、布莱斯•帕斯卡（Blaise Pascal）、雅各布·伯努利（Jakob Bernoulli）、亚伯拉罕·棣莫弗（Abraham de Moivre）、托马斯·贝叶斯（Thomas Bayes） 和 理查德·普莱斯（Richard Price）等人的工作奠定了概率论的基础，到了 19 世纪，许多统计学家开始使用概率分布作为其分析工具包的一部分。

数学的这些新发展使统计学家能够超越描述性统计并开始进行统计学习。 皮埃尔·西蒙·德·拉普拉斯（Pierre Simon de Laplace） 和 -   卡尔·弗里德里希·高斯（Carl Friedrich Gauss） 是 19 世纪最重要和最著名的两位数学家，他们都对统计学习和现代数据科学做出了重要贡献。拉普拉斯借鉴了托马斯·贝叶斯和理查德·普莱斯的直觉，并将它们发展成为我们现在称为贝叶斯法则的第一个版本。高斯在寻找失踪的矮行星谷神星时，开发了最小二乘法。最小二乘法为线性回归和逻辑回归等统计学习方法以及人工智能中人工神经网络模型的发展奠定了基础。

1780 年至 1820 年间，大约在拉普拉斯和高斯为统计学习做出贡献的同时，一位名叫威廉·普莱费尔的苏格兰工程师发明了统计图形，并为现代数据可视化和探索性数据分析奠定了基础。 Playfair 发明了用于时间序列数据的折线图和面积图，用条形图说明不同类别数量之间的比较，用饼图说明一组内的比例。可视化定量数据的优势在于它允许我们利用我们强大的视觉能力来总结、比较和解释数据。诚然，大型（多数据点）或复杂（多属性）数据集很难可视化，但数据可视化仍然是数据科学的重要组成部分。特别是，它有助于帮助数据科学家探索和理解他们正在使用的数据。可视化对于展示数据科学项目的结果也很有用。自 Playfair 时代以来，数据可视化图形的种类一直在稳步增长，如今正在研究开发可视化大型多维数据集的新方法。最近的一项发展是 t 分布随机邻域嵌入 (t-SNE) 算法，它是一种有用的技术，可以将高维数据减少到二维或三个维度，从而使得这些数据的可视化成为可能。

概率论和统计学的发展一直持续到 20 世纪。 Karl Pearson 开发了现代假设检验，R. A. Fisher 开发了用于多元分析的统计方法，并将最大似然估计的概念引入统计推断中，作为基于事件相对概率得出结论的方法。艾伦·图灵在第二次世界大战中的工作导致了电子计算机的发明，这对统计产生了巨大的影响，因为它可以进行更复杂的统计计算。在整个 1940 年代和随后的几十年中，开发了许多重要的计算模型，这些模型仍然广泛用于数据科学。 1943 年，Warren McCulloch 和 Walter Pitts 提出了第一个神经网络的数学模型。 1948 年，克劳德·香农发表了《通信的数学理论》，并由此创立了信息论。 1951 年，Evelyn Fix 和 Joseph Hodges 提出了一个判别分析模型（现在称为分类或模式识别问题），该模型成为现代最近邻模型的基础。 1956 年，这些战后发展在达特茅斯学院的一个研讨会上建立了人工智能领域。即使在人工智能发展的早期阶段，机器学习这个术语也开始被用来描述使计算机能够从数据中学习的程序。在 1960 年代中期，对 ML 做出了三项重要贡献。 1965 年，尼尔斯·尼尔森 (Nils Nilsson) 的书《学习机器》(Learning Machines) 展示了如何使用神经网络来学习用于分类的线性模型。次年，即 1966 年，Earl B. Hunt、Janet Marin 和 Philip J. Stone 开发了概念学习系统框架，它是一个重要的 ML 算法家族的祖先，这些算法从数据中导出决策树模型以自上而下的方式。大约在同一时间，许多独立研究人员开发并发布了 k-means 聚类算法的早期版本，现在是用于数据（客户）分割的标准算法。

ML 领域是现代数据科学的核心，因为它提供了能够自动分析大型数据集以提取潜在有趣和有用模式的算法。直到今天，机器学习一直在不断发展和创新。一些最重要的发展包括集成模型，其中使用一组模型（或委员会）进行预测，每个模型对每个查询进行投票，以及深度学习神经网络，它具有多个（即超过三个) 神经元层。网络中的这些更深层能够发现和学习复杂的属性表示（由多个已由早期层处理的交互输入属性组成），这反过来又使网络能够学习在输入中泛化的模式数据。由于能够学习复杂属性，深度学习网络特别适用于高维数据，因此彻底改变了许多领域，包括机器视觉和自然语言处理。

正如我们在回顾数据库历史时所讨论的那样，1970 年代初期标志着现代数据库技术的开端，Edgar F. Codd 的关系数据模型以及随后数据生成和存储的爆炸式增长导致了 1990 年代及以后数据仓库的发展最近到了大数据的现象。然而，早在大数据出现之前，实际上到 1980 年代末和 1990 年代初，对专门针对这些大数据集分析的研究领域的需求就很明显了。大约在这个时候，数据挖掘这个术语开始在数据库社区中使用。正如我们已经讨论过的，对这种需求的一种回应是开发数据仓库。然而，其他数据库研究人员通过接触其他研究领域做出回应，1989 年 Gregory Piatetsky-Shapiro 组织了第一次关于数据库中知识发现 (KDD) 的研讨会。第一届 KDD 研讨会的宣布巧妙地总结了研讨会如何专注于分析大型数据库问题的多学科方法：数据库中的知识发现提出了许多有趣的问题，尤其是当数据库很大。这样的数据库通常伴随着通过大量的领域知识可以显着促进发现。访问大型数据库的成本很高——因此需要抽样和其他统计方法。最后，数据库中的知识发现可以受益于多个不同领域的许多可用工具和技术，包括专家系统、机器学习、智能数据库、知识获取和统计。

事实上，数据库中的知识发现和数据挖掘这两个术语描述了相同的概念，区别在于数据挖掘在商业社区中更为普遍，而 KDD 在学术社区中更为普遍。今天，这些术语经常可以互换使用，2 并且许多顶级学术场所同时使用这两种术语。事实上，该领域最重要的学术会议是知识发现和数据挖掘国际会议。

# 数据科学的出现和发展
数据科学一词在 1990 年代后期在与统计学家与计算机科学家一起为大型数据集的计算分析带来数学严谨性的讨论中变得突出。 1997 年，C. F. Jeff Wu 的公开演讲“统计 = 数据科学？”强调了一些有希望的统计趋势，包括大型数据库中大型/复杂数据集的可用性以及计算算法和模型的日益使用。他在演讲结束时呼吁将统计学重新命名为“数据科学”。

2001 年，William S. Cleveland 发布了一项在数据科学领域创建大学部门的行动计划（Cleveland 2001）。该计划强调数据科学需要成为数学和计算机科学之间的伙伴关系。它还强调需要将数据科学理解为一项多学科的努力，并强调数据科学家需要学习如何与主题专家合作和互动。同年，Leo Breiman 发表了《统计建模：两种文化》（2001 年）。在本文中，Breiman 将传统的统计方法描述为一种数据建模文化，它将数据分析的主要目标视为识别解释数据如何生成的（隐藏的）随机数据模型（例如线性回归）。他将这种文化与算法建模文化进行了对比，算法建模文化专注于使用计算机算法来创建准确的预测模型（而不是解释如何生成数据）。 Breiman 将统计重点放在解释数据的模型与算法重点放在可以准确预测数据的模型之间的区别突出了统计学家和机器学习研究人员之间的核心差异。这些方法之间的争论仍在统计中进行（例如，参见 Shmueli 2010）。一般来说，今天大多数数据科学项目更倾向于使用 ML 方法来构建准确的预测模型，而不太关心解释数据的统计重点。因此，尽管数据科学在与统计有关的讨论中变得突出，并且仍然从统计中借鉴方法和模型，但随着时间的推移，它已经发展出自己独特的数据分析方法。

自2001年以来，数据科学的概念已经远远超出 了重新定义统计学的范畴。例如，在过去10年中，在线活动(在线零售、社 交媒体和在线娱乐)生成的数据量有了巨大的增 长。收集和准备这些数据以用于数据科学项目， 导致数据科学家需要开发编程和黑客技能，以从 外部Web源抓取、合并和清理数据(有时是非结构 化数据)。此外，大数据的出现意味着数据科学 家需要能够使用大数据技术，如Hadoop。事实上， 如今数据科学家的角色已经变得如此广泛，以至 于关于如何定义履行这一角色所需的专业知识和 技能的争论一直在持续。3但是，可以列出大多数 人认为与该角色相关的专业知识和技能，如图1所 示。个人很难掌握所有这些领域，事实上， 大多 数数据科学家通常只在其中的一个子集上拥有深 入的知识和真正的专业技能。但是，了解并意识 到每个领域对数据科学项目的贡献非常重要。

数据科学家应该具备一些领域专业知识。大多 数数据科学项目都是从一个真实的、特定领域的 问题开始的，并且需要为这个问题设计一个数据 驱动的解决方案。因此，对于数据科学家来说， 拥有足够的领域专业知识是非常重要的，这样他 们才能理解问题、问题的重要性以及问题的数据 科学解决方案如何适应组织的流程。这一领域的专业知识指导数据科学 家确定优化的解决方案。这也使她能够以一种有 意义的方式与真正的领域专家接触，这样她就可 以了解关于潜在问题的相关知识。此外，拥有项 目领域的一些经验使数据科学家能够将其在相同 和相关领域从事类似项目的经验用于定义项目重点和范围。

数据是所有数据科学项目的核心。然而，一个 组织有权访问数据并不意味着它可以合法地或应 该合乎道德地使用数据。在大多数司法管辖区， 都有反歧视和个人数据保护立法来规范和控制数 据的使用。因此，数据科学家需要了解这些法规， 并且更广泛地说，如果他要合法和适当地使用数 据，就需要对其工作的影响有道德上的理解。我 们将在第6章回到这一主题，讨论有关数据使用的 法律法规以及与数据科学相关的伦理问题。

在大多数组织中，很大一部分数据将来自组织 中的数据库。此外，随着组织数据架构的增长， 数据科学项目将开始整合来自各种其他数据源的 数据，这些数据源通常称为“大数据源”。这些数据 中的数据源可以以各种不同的格式存在，通常是某种形式 的数据库—关系型、NoSQL或Ha-doop。这些不同 数据库和数据源中的所有数据都需要进行集成、 清理、转换、规范化等。这些任务有许多名称， 例如提取、转换和加载、“数据整理”、“数据争论”、 “数据融合”、“处理数据”等等。与源数据一样，数 据科学活动产生的数据也需要存储和管理。同样， 数据库是这些活动生成的数据的典型存储位置， 因为它们可以很容易地分发给组织的不同部门并 与之共享。因此，数据科学家需要具备与数据库 中的数据进行交互和操作的技能。

一系列计算机科学技能和工具使数据科学家能 够处理大数据，并将其处理为新的、有意义的信 息。高性能计算(HPC)涉及聚合计算能力，以提 供比独立计算机更高的性能。许多数据科学项目 使用非常大的数据集和计算成本高昂的ML算法。 在这些情况下，掌握访问和使用HPC资源所需的技 能非常重要。除了HPC之外，我们已经提到了数据 科学家需要能够废弃、清理和集成Web数据，以及 处理非结构化文本和图像。此外，数据科学家也 可能最终编写内部应用程序以执行特定任务，或更改现有 应用程序以根据正在处理的数据和域对其进行调 整。最后，还需要计算机科学技能来理解和开发 ML模型，并将其集成到组织中的生产或分析或后 端应用程序中。

以图形格式显示数据可以更容易地查看和理解 数据发生的情况。数据可视化适用于数据科学流 程的所有阶段。当以表格形式检查数据时，很容 易遗漏诸如分布中的异常值或趋势或数据随时间 的细微变化等内容。但是，当数据以正确的图形 形式呈现时，数据的这些方面可能会突出显示。 数据可视化是一个重要且不断发展的领域，我们 推荐两本书:爱德华·塔夫特(Edward Tufte， 2001 ) 的 《 定 量 信 息 的 可 视 化 显 示 》 ( The Visual Display of Quantitative Information ) 和斯蒂芬·菲尤(Stephen Few，2012)的《向我展 示数字:设计表格和图形来启发》(Show Me the Numbers : Designing Tables and Graphs to Enlighten)，这两本书是对有效数据可视化的原 理和技术的极好介绍。

在整个数据科学过程中，从最初的数据收集 和调查一直到比较项目期间产生的不同模型和 分析的结果，都使用了统计和概率方法。机器 学习涉及使用各种先进的统计和计算技术来处 理数据，以找到模式。这个参与ML应用方面的数据科学家不必编写自己的ML 算法版本。通过了解ML算法，它们可以用于什么， 它们生成的结果意味着什么，以及特定算法可以 在什么类型的数据上运行，数据科学家可以将ML 算法视为一个灰盒。这使他能够专注于数据科学 的应用方面，并测试各种ML算法，以了解哪些算 法最适合他所关注的场景和数据。

最后，成为一名成功的数据科学家的一个关键 方面是能够传达数据中的故事。这个故事可能会 揭示数据分析所揭示的洞察力，或者在项目期间 创建的模型如何适应组织的流程，以及它们可能 对组织的功能产生的影响。执行一个出色的数据 科学项目是没有意义的，除非使用它的输出，并 以非技术背景的同事能够理解并信任的方式传达 结果。

# 数据科学用在哪里?
数据科学推动着现代社会几乎所有领域的决策制 定。在本节中，我们将介绍三种情况说明数据科学影响的研究:消费者公司使用数据 科学进行销售和营销;政府利用数据科学改善健 康、刑事司法和城市规划;以及在球员招募中使 用数据科学的职业体育特许经营。

## 销售和营销中的数据科学
通过使用销售点系统、跟踪顾客在沃尔玛网站上 的行为以及跟踪有关沃尔玛及其产品的社交媒体 评论，沃尔玛可以访问有关其顾客偏好的大型数 据集。十多年来，沃尔玛一直在使用数据科学来 优化商店的库存水平，一个著名的例子是，根据 对几周前袭击的飓风“查理”(Charley)之前的销 售数据的分析，沃尔玛在2004年飓风“弗朗西斯” (Francis)经过的商店中补充了草莓馅饼。最近， 沃尔玛利用数据科学推动其零售收入，在分析社 交媒体趋势的基础上推出新产品，分析信用卡活 动以向客户推荐产品，并优化和个性化客户在沃 尔玛网站上的在线体验。沃尔玛将10%至15%的在 线 销 售 额 增 长 归 功 于 数 据 科 学 优 化 ( Dezyre 2015)。

在网络世界中，相当于追加销售和交叉销售的 是“推荐系统”。如果你有如果你在Netflix上看了一部电影，或者在亚马逊 上购买了一件商品，你就会知道这些网站会使用 他们收集的数据来为你下一步应该看什么或买什 么提供建议。这些推荐系统可以被设计成以不同 的方式引导你:一些引导你去看大片和畅销书， 而另一些则引导你去看适合你口味的小众商品。 克里斯·安德森(Chris Anderson)在《长尾理论》 (The Long Tail，2008)一书中指出，随着生产 和分销成本的降低，市场从大量销售少量畅销商 品转变为少量销售大量小众商品。这种在推动热 门产品或利基产品销售之间的权衡是推荐系统的 基本设计决策，并影响用于实现这些系统的数据 科学算法。

## 政府使用数据科学
近年来，各国政府已经认识到采用数据科学的优 势。例如，2015年，美国政府任命D.J.Patil博士 为首位首席数据科学家。由美国政府牵头的一些 最大的数据科学计划一直在健康领域。数据科学 是癌症登月计划4和精准医疗计划的核心。精准医 疗计划将人类基因组测序和数据科学相结合，为 个体患者设计药物。该倡议的一部分是“我们所 有人”计划5，从100多万名志愿者那里收集环境、生活方式和生 物数据，创建世界上最大的精准医疗数据集。数 据科学也在彻底改变我们组织城市的方式:它被 用于跟踪、分析和控制环境、能源和交通系统， 并为长期城市规划提供信息(Kitchin 2014a)。 在第7章中，当我们讨论未来几十年数据科学将如 何在我们的生活中变得更加重要时，我们将回到 健康和智能城市。

美国政府的警察数据计划6专注于使用数据科学 来帮助警察部门了解其社区的需求。数据科学也 被用于预测犯罪热点和累犯。然而，公民自由团 体批评了数据科学在刑事司法中的一些用途。在 第6章中，我们讨论了数据科学提出的隐私和道德 问题，其中一个有趣的因素是，人们对个人隐私 和数据科学的看法因领域而异。许多人很高兴他 们的个人数据被用于公共资助的医学研究，但当 涉及到将个人数据用于警务和刑事司法时，他们 有非常不同的意见。在第6章中，我们还讨论了如 何使用个人数据和数据科学来确定人寿、健康、 汽车、家庭和旅行保险费。

## 职业体育中的数据科学
由布拉德·皮特(Brad Pitt)主演的电影《点球成 金》(Moneyball，Bennett Miller，2011)展示 了数据科学在现代体育中越来越多的应用。这部 电影改编自同名小说(Lewis 2004)，讲述了奥 克兰运动家队(Oakland A)棒球队如何利用数据 科学改善其球员招募的真实故事。球队管理层发 现，球员的上垒率和长打率统计数据比传统棒球 中强调的统计数据(如球员的击球率)更能提供 进攻成功的信息。这种洞察力使奥克兰运动家队 能够招募一批被低估的球员，并超越其预算。奥 克兰运动家队在数据科学方面的成功彻底改变了 棒球运动，大多数其他棒球队现在都将类似的数 据驱动策略整合到他们的招募流程中。

《点球成金》的故事就是一个非常明显的例子。 数据科学可以让组织在竞争激烈的市场空间中 获得优势。然而，从纯数据科学的角度来看， 也许《点球成金》故事最重要的方面是，它强 调了有时数据科学的主要价值是识别信息属性。 人们普遍认为，数据科学的价值在于通过该过 程创建的模型。然而，一旦我们知道了域中的 重要属性，就很容易创建数据驱动的模型。成 功的关键是获得正确的数据和发现正确的属性。史蒂文·D·莱维特(Steven D.Levitt) 和斯蒂芬·杜布纳(Stephen Dubner)在《魔鬼经 济学:一个流氓经济学家探索一切事物的隐藏面》 ( Freakonomics : A Rogue Economist Explore the Hidden Side of Everything)一书中阐述了 这一观察在一系列广泛问题中的重要性。正如他 们所说，理解现代生活的关键是“知道衡量什么和 如何衡量”(2009，14)。使用数据科学，我们可 以发现数据集中的重要模式，而这些模式可以揭 示领域中的重要属性。数据科学之所以被用于如 此多的领域，是因为问题领域是什么并不重要: 如果有正确的数据，并且可以明确定义问题，那 么数据科学就可以提供帮助。

## 为什么是现在?
许多因素促成了最近数据科学的发展。正如我们 已经提到的，大数据的出现是由组织可以相对轻 松地收集数据所推动的。无论是通过PointOf- Sales交易记录、在线平台的点击、社交媒体帖子、 智能手机上的应用程序，还是无数其他渠道，公 司现在都可以建立更丰富的个人客户档案。另一 个因素是具有规模经济的数据存储的商品化，这 使得存储数据的成本比以往任何时候都低。计算机的能力也有了巨大的增长。图形卡和图形处理单元(GPU)最初是为计算机游戏进 行快速图形渲染而开发的。GPU的显著特点是它们 可以执行快速矩阵乘法。然而，矩阵乘法不仅对 图形渲染有用，而且对ML也有用。近年来，GPU已 经针对ML的使用进行了调整和优化，这大大加快 了数据处理和模型训练的速度。用户友好的数据 科学工具也变得可用，并降低了进入数据科学的 门槛。总之，这些发展意味着收集、存储和处理 数据从未如此简单。

在过去10年中，ML也取得了重大进展。特别是， 深度学习已经出现，并彻底改变了计算机处理语 言和图像数据的方式。“深度学习”一词描述了一系 列神经网络模型，网络中有多层单元。神经网络 早在20世纪40年代就出现了，但它们最适合处理 大型、复杂的数据集，并且需要大量的计算资源 来训练。因此，深度学习的出现与大数据和计算 能力的增长有关。可以毫不夸张地说，深度学习 对一系列领域的影响是非凡的。

DeepMind的计算机程序AlphaGo7是一个很好的 例子，说明了深度学习如何改变了研究领域。围棋是一种棋盘游戏，起源于3000年 前的中国。围棋的规则比国际象棋简单得多。玩 家轮流在棋盘上放置棋子，目标是夺取对手的棋 子或包围空的区域。然而，规则的简单性和围棋 使用更大棋盘的事实意味着围棋中比国际象棋中 有更多可能的棋盘配置。事实上，围棋可能的棋 盘配置比宇宙中的原子还要多。对于计算机来说， 这使得围棋比国际象棋困难得多，因为它的搜索 空间大得多，并且难以评估这些可能的棋盘配置 中的每一个。DeepMind团队使用深度学习模型使 AlphaGo能够评估棋盘配置并选择下一步行动。结 果是AlphaGo成为第一个击败职业围棋选手的计算 机程序，2016年3月AlphaGo击败了LED Sedol。这 位18次围棋世界冠军， 这场比赛全世界有超过2亿 人观看。将深度学习对围棋的影响放在大背景下 来看，就在2009年，世界上最好的围棋计算机程 序还被评为“高级业余”的低端。七年后，AlphaGo 击败了世界冠军。2016年，一篇描述AlphaGo背后 的深度学习算法的文章发表在世界上最负盛名的 学术科学杂志《自然》上(Silver，Huang， Maddison，et al。2016).

深度学习还对一系列备受瞩目的消费技术产生 了巨大影响。Facebook现在使用深度学习进行面 部识别和文本分析，以便根据个人的在线对话直 接向他们做广告。谷歌和百度都将深度学习用于 图像识别、字幕和搜索以及机器翻译。苹果的虚 拟助手Siri、亚马逊的Alexa、微软的Cortana和 三星的Bixby都使用了基于深度学习的语音识别。 华为目前正在为中国市场开发一款虚拟助手，它 也将使用深度学习语音识别。在第4章“机器学习 101”中，我们更详细地描述了神经网络和深度学习。 然而，尽管深度学习是一项重要的技术发展， 就 数据科学的发展而言，最重要的也许是人们对数 据科学的能力和优势的认识有所提高，以及这些 备受瞩目的成功案例所带来的组织认同。

## 关于数据科学的神话
数据科学对现代组织有很多好处，但也有大量的 炒作，所以我们应该了解它的局限性是什么。最 大的神话之一是相信数据科学是一种自主的我们可以利用数据来寻找问题的答案。在现实中， 数据科学需要熟练的人员在整个流程的不同阶段 进行监督。人类分析师需要构建问题，设计和准 备数据，选择最合适的ML算法，批判性地解释分 析结果，并根据分析揭示的见解计划采取适当的 行动。如果没有熟练的人力监督，数据科学项目 将无法实现其目标。当人类的专业知识和计算机 的能力共同发挥作用时，最好的数据科学成果就 会出现，正如戈登·林诺夫(Gordon Linoff)和迈 克尔·贝里(Michael Berry)所说:数据挖掘让计 算机做它们最擅长的事情——挖掘大量数据。这 反过来又让人们做人们最擅长的事情，即设置问 题并理解结果(2011，3)。

数据科学的使用越来越广泛，这意味着如今许 多组织面临的最大数据科学挑战是找到合格的人 类分析师并雇用他们。数据科学领域的人才非常 宝贵，而寻找这些人才是目前采用数据科学的主 要瓶颈。2011年，麦肯锡全球研究所(McKinsey Global Institute)的一份报告预测，美国拥有 数据科学和分析技能的人才缺口在140，000至190， 000人之间，更大的缺口是 150万具有数据理解能力的管理者科学和分析过程的水平将使他们能够适当地询问 和解释数据科学的结果(Manyika，Chui，Brown 等人2011).五年过去了，在2016年的报告中，该 研究所仍然坚信，数据科学在不断扩大的应用范 围内具有巨大的未开发价值潜力，但人才短缺将 继续存在，预计短期内将短缺25万名数据科学家 (Henke、Bughin、Chui等人)。2016).

数据科学的第二大迷思是，每一个数据科学项 目都需要大数据，都需要用到深度学习。一般来 说，拥有更多的数据是有帮助的，但拥有正确的 数据是更重要的要求。与谷歌、百度或微软相比， 数据科学项目经常在数据和计算能力资源明显较 少的组织中开展。指示较小数据科学项目规模的 示例包括:保险公司的索赔预测，该公司每月处 理大约100起索赔;学生规模不足1万人的高校学 生流失预测拥有数千名会员的工会的会员流失预 测。因此，组织不需要处理数TB的数据，也不需 要拥有大量的计算资源，就可以从数据科学中获 益。

第三个数据科学神话是，现代数据科学软件易 于使用，因此数据科学易于做。的确，数据科学软件已经变得更加用户友好。 然而，这种易用性可能会掩盖这样一个事实，即 正确地进行数据科学既需要适当的领域知识，也 需要关于数据属性和支持不同ML算法的假设的专 业知识。事实上，把数据科学做坏从来没有这么 容易过。就像生活中的其他事情一样，如果你在 做数据科学时不明白自己在做什么，你就会犯错 误。数据科学的危险在于，人们可能会被技术吓 倒，并相信软件呈现给他们的任何结果。然而， 他们可能无意中以错误的方式界定了问题，输入 了错误的数据，或者使用了带有不恰当假设的分 析技术。因此，软件呈现的结果很可能是错误问 题的答案，或者是基于错误的数据或错误计算的 结果。

我们在这里要提到的关于数据科学的最后一个 神话是相信数据科学很快就能收回成本。这种信 念的真实性取决于组织的背景。采用数据科学可 能需要在开发数据基础设施和雇用具有数据科学 专业知识的员工方面进行大量投资。此外，数据 科学不会在每个项目上都产生积极的结果。有时 数据中没有隐藏的真知灼见，有时，组织无法根据分析所揭示的见解采取行动。 然而，在业务问题得到充分理解且具备适当的数 据和人类专业知识的情况下，数据科学(通常) 可以提供可操作的见解，为组织提供成功所需的 竞争优势。




# 前言
Python 具有的如下几个特征使得它特别适合用来学习数据科学。
+ Python 是免费的。
+ 使用 Python 进行编程是相对容易的。
+ 有许多有用的数据科学相关的库。
# 第一章 介绍
## 1. 数据无处不在
我们生活在一个被数据淹没的世界。网站记录每个用户的每次点击；你的智能手机每时每刻都在记录着你的位置和移动速度；智能汽车收集司机的驾驶习惯；智能家居收集主人的生活习惯；聪明的营销人员收集客户的购买习惯；

## 2. 什么是数据科学？
[Data Scientist: The Sexiest Job of the 21st Century](https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century)
## 3. 数据科学的历史
### 3.1 数据收集的历史
### 3.2 数据分析的历史
### 3.3 数据科学的出现和发展
## 4. 数据科学用在什么地方？
### 4.1 销售
### 4.2 政府
### 4.3 职业体育
## 5. 为什么是现在？
### 5.1 关于数据科学的神话
## 6. 数据科学的工作内容


https://www.simplilearn.com/tutorials/data-science-tutorial/introduction-to-data-science
https://www.youtube.com/watch?v=-ETQ97mXXF0
## 1. 什么是数据科学？谁是数据科学家？
## 2. 数据科学家所需要的技能
## 3. 数据科学家做哪些事情？
### 3.1 数据获取
### 3.2 数据准备
### 3.3 数据挖掘
### 3.4 模型构建和测试
### 3.5 模型维护
## 4. 总结

---
## What is data science?
 
> Data science is the study of data to extract meaningful insights for business. It is a multidisciplinary approach that combines principles and practices from the fields of mathematics, statistics, artificial intelligence, and computer engineering to analyze large amounts of data. This analysis helps data scientists to ask and answer questions like what happened, why it happened, what will happen, and what can be done with the results.  

数据科学是指从大量的数据中抽取出有用的、并非显而易见的知识并用于决策。

## Why is data science important?

> Data science is important because it combines tools, methods, and technology to generate meaning from data. Modern organizations are inundated with data; there is a proliferation of devices that can automatically collect and store information. Online systems and payment portals capture more data in the fields of e-commerce, medicine, finance, and every other aspect of human life. We have text, audio, video, and image data available in vast quantities.    

> Unfortunately, raw data is of no value unless it can be actioned. Data scientists can convert raw data into meaningful recommendations. They can uncover and solve problems that businesses did not even know existed. Organizations can use these recommendations to make customers happier, optimize the supply chain, or launch new products.  

只有能够指导行动的数据才能称为知识。

## History of data science

While the term data science is not new, the meanings and connotations have changed over time. The word first appeared in the ’60s as an alternative name for statistics. In the late ’90s, computer science professionals formalized the term. A proposed definition for data science saw it as a separate field with three aspects: data design, collection, and analysis. It still took another decade for the term to be used outside of academia.   

## Future of data science

Artificial intelligence and machine learning innovations have made data processing faster and more efficient. Industry demand has created an ecosystem of courses, degrees, and job positions within the field of data science. Because of the cross-functional skillset and expertise required, data science shows strong projected growth over the coming decades.  

## What is data science used for?

Data science is used to study data in four main ways:  

### 1. Descriptive analysis

Descriptive analysis examines data to gain insights into what happened or what is happening in the data environment. It is characterized by data visualizations such as pie charts, bar charts, line graphs, tables, or generated narratives. For example, a flight booking service may record data like the number of tickets booked each day. Descriptive analysis will reveal booking spikes, booking slumps, and high-performing months for this service.  

### 2. Diagnostic analysis

Diagnostic analysis is a deep-dive or detailed data examination to understand why something happened. It is characterized by techniques such as drill-down, data discovery, data mining, and correlations. Multiple data operations and transformations may be performed on a given data set to discover unique patterns in each of these techniques.For example, the flight service might drill down on a particularly high-performing month to better understand the booking spike. This may lead to the discovery that many customers visit a particular city to attend a monthly sporting event.  

### 3. Predictive analysis

Predictive analysis uses historical data to make accurate forecasts about data patterns that may occur in the future. It is characterized by techniques such as machine learning, forecasting, pattern matching, and predictive modeling. In each of these techniques, computers are trained to reverse engineer causality connections in the data.For example, the flight service team might use data science to predict flight booking patterns for the coming year at the start of each year. The computer program or algorithm may look at past data and predict booking spikes for certain destinations in May. Having anticipated their customer’s future travel requirements, the company could start targeted advertising for those cities from February.  

### 4. Prescriptive analysis

Prescriptive analytics takes predictive data to the next level. It not only predicts what is likely to happen but also suggests an optimum response to that outcome. It can analyze the potential implications of different choices and recommend the best course of action. It uses graph analysis, simulation, complex event processing, neural networks, and recommendation engines from machine learning.         

Back to the flight booking example, prescriptive analysis could look at historical marketing campaigns to maximize the advantage of the upcoming booking spike. A data scientist could project booking outcomes for different levels of marketing spend on various marketing channels. These data forecasts would give the flight booking company greater confidence in their marketing decisions.  

## What are the benefits of data science for business?

Data science is revolutionizing the way companies operate. Many businesses, regardless of size, need a robust data science strategy to drive growth and maintain a competitive edge. Some key benefits include:

### Discover unknown transformative patterns

Data science allows businesses to uncover new patterns and relationships that have the potential to transform the organization. It can reveal low-cost changes to resource management for maximum impact on profit margins.For example, an e-commerce company uses data science to discover that too many customer queries are being generated after business hours. Investigations reveal that customers are more likely to purchase if they receive a prompt response instead of an answer the next business day. By implementing 24/7 customer service, the business grows its revenue by 30%.  

### Innovate new products and solutions

Data science can reveal gaps and problems that would otherwise go unnoticed. Greater insight about purchase decisions, customer feedback, and business processes can drive innovation in internal operations and external solutions.For example, an online payment solution uses data science to collate and analyze customer comments about the company on social media. Analysis reveals that customers forget passwords during peak purchase periods and are unhappy with the current password retrieval system. The company can innovate a better solution and see a significant increase in customer satisfaction.  

### Real-time optimization

It’s very challenging for businesses, especially large-scale enterprises, to respond to changing conditions in real-time. This can cause significant losses or disruptions in business activity. Data science can help companies predict change and react optimally to different circumstances.For example, a truck-based shipping company uses data science to reduce downtime when trucks break down. They identify the routes and shift patterns that lead to faster breakdowns and tweak truck schedules. They also set up an inventory of common spare parts that need frequent replacement so trucks can be repaired faster.    

## What is the data science process?

A business problem typically initiates the data science process. A data scientist will work with business stakeholders to understand what business needs. Once the problem has been defined, the data scientist may solve it using the OSEMN data science process:

### O – Obtain data

Data can be pre-existing, newly acquired, or a data repository downloadable from the internet. Data scientists can extract data from internal or external databases, company CRM software, web server logs, social media or purchase it from trusted third-party sources.  

### S – Scrub data

Data scrubbing, or data cleaning, is the process of standardizing the data according to a predetermined format. It includes handling missing data, fixing data errors, and removing any data outliers. Some examples of data scrubbing are:· 

-   Changing all date values to a common standard format.·  
-   Fixing spelling mistakes or additional spaces.·  
-   Fixing mathematical inaccuracies or removing commas from large numbers.  
    

### E – Explore data

Data exploration is preliminary data analysis that is used for planning further data modeling strategies. Data scientists gain an initial understanding of the data using descriptive statistics and data visualization tools. Then they explore the data to identify interesting patterns that can be studied or actioned.        

### M – Model data

Software and machine learning algorithms are used to gain deeper insights, predict outcomes, and prescribe the best course of action. Machine learning techniques like association, classification, and clustering are applied to the training data set. The model might be tested against predetermined test data to assess result accuracy. The data model can be fine-tuned many times to improve result outcomes.   

### N – Interpret results

Data scientists work together with analysts and businesses to convert data insights into action. They make diagrams, graphs, and charts to represent trends and predictions. Data summarization helps stakeholders understand and implement results effectively.  

## What are the data science techniques?

Data science professionals use computing systems to follow the data science process. The top techniques used by data scientists are:

### Classification

Classification is the sorting of data into specific groups or categories. Computers are trained to identify and sort data. Known data sets are used to build decision algorithms in a computer that quickly processes and categorizes the data. For example:·  

-   Sort products as popular or not popular·  
-   Sort insurance applications as high risk or low risk·  
-   Sort social media comments into positive, negative, or neutral.

Data science professionals use computing systems to follow the data science process.   

### Regression

Regression is the method of finding a relationship between two seemingly unrelated data points. The connection is usually modeled around a mathematical formula and represented as a graph or curves. When the value of one data point is known, regression is used to predict the other data point. For example:·  

-   The rate of spread of air-borne diseases.· 
-    The relationship between customer satisfaction and the number of employees.·  
-   The relationship between the number of fire stations and the number of injuries due to fire in a particular location.   
    

### Clustering

Clustering is the method of grouping closely related data together to look for patterns and anomalies. Clustering is different from sorting because the data cannot be accurately classified into fixed categories. Hence the data is grouped into most likely relationships. New patterns and relationships can be discovered with clustering. For example: ·  

-   Group customers with similar purchase behavior for improved customer service.·  
-   Group network traffic to identify daily usage patterns and identify a network attack faster.  
-   Cluster articles into multiple different news categories and use this information to find fake news content.  
    

### The basic principle behind data science techniques

While the details vary, the underlying principles behind these techniques are:

-   Teach a machine how to sort data based on a known data set. For example, sample keywords are given to the computer with their sort value. “Happy” is positive, while “Hate” is negative.
-   Give unknown data to the machine and allow the device to sort the dataset independently.
-    Allow for result inaccuracies and handle the probability factor of the result.  

## What are different data science technologies?

Data science practitioners work with complex technologies such as:

1.  **Artificial intelligence:** Machine learning models and related software are used for predictive and prescriptive analysis.
2.  **Cloud computing:** Cloud technologies have given data scientists the flexibility and processing power required for advanced data analytics.
3.  **Internet of things:** IoT refers to various devices that can automatically connect to the internet. These devices collect data for data science initiatives. They generate massive data which can be used for data mining and data extraction.
4.  **Quantum computing:** Quantum computers can perform complex calculations at high speed. Skilled data scientists use them for building complex quantitative algorithms.

## How does data science compare to other related data fields?

Data science is an all-encompassing term for other data-related roles and fields. Let’s look at some of them here:  

### What is the difference between data science and data analytics?

While the terms may be used interchangeably, data analytics is a subset of data science. Data science is an umbrella term for all aspects of data processing—from the collection to modeling to insights. On the other hand, data analytics is mainly concerned with statistics, mathematics, and statistical analysis. It focuses on only data analysis, while data science is related to the bigger picture around organizational data.In most workplaces, data scientists and data analysts work together towards common business goals. A data analyst may spend more time on routine analysis, providing regular reports. A data scientist may design the way data is stored, manipulated, and analyzed. Simply put, a data analyst makes sense out of existing data, whereas a data scientist creates new methods and tools to process data for use by analysts.

### What is the difference between data science and business analytics?

While there is an overlap between data science and business analytics, the key difference is the use of technology in each field. Data scientists work more closely with data technology than business analysts.Business analysts bridge the gap between business and IT. They define business cases, collect information from stakeholders, or validate solutions. Data scientists, on the other hand, use technology to work with business data. They may write programs, apply machine learning techniques to create models, and develop new algorithms. Data scientists not only understand the problem but can also build a tool that provides solutions to the problem.It’s not unusual to find business analysts and data scientists working on the same team. Business analysts take the output from data scientists and use it to tell a story that the broader business can understand.

### What is the difference between data science and data engineering?

Data engineers build and maintain the systems that allow data scientists to access and interpret data. They work more closely with underlying technology than a data scientist. The role generally involves creating data models, building data pipelines, and overseeing extract, transform, load (ETL). Depending on organization setup and size, the data engineer may also manage related infrastructure like big-data storage, streaming, and processing platforms like Amazon S3.Data scientists use the data that data engineers have processed to build and train predictive models. Data scientists may then hand over the results to the analysts for further decision making.

### What is the difference between data science and machine learning?

learning?Machine learning is the science of training machines to analyze and learn from data the way humans do. It is one of the methods used in data science projects to gain automated insights from data. Machine learning engineers specialize in computing, algorithms, and coding skills specific to machine learning methods. Data scientists might use machine learning methods as a tool or work closely with other machine learning engineers to process data.

### What is the difference between data science and statistics? 

Statistics is a mathematically-based field that seeks to collect and interpret quantitative data. In contrast, data science is a multidisciplinary field that uses scientific methods, processes, and systems to extract knowledge from data in various forms. Data scientists use methods from many disciplines, including statistics. However, the fields differ in their processes and the problems they study.    

## What are different data science tools?

AWS has a range of tools to support data scientists around the globe:

### Data storage

For data warehousing, [Amazon Redshift](https://aws.amazon.com/redshift/) can run complex queries against structured or unstructured data. Analysts and data scientists can use [AWS Glue](https://aws.amazon.com/glue/) to manage and search for data. AWS Glue automatically creates a unified catalog of all data in the data lake, with metadata attached to make it discoverable.  

### Machine learning

[Amazon SageMaker](https://aws.amazon.com/sagemaker/) is a fully-managed machine learning service that runs on the Amazon Elastic Compute Cloud (EC2). It allows users to organize data, build, train and deploy machine learning models, and scale operations.

### Analytics

-    [Amazon Athena](https://aws.amazon.com/athena/) is an interactive query service that makes it easy to analyze data in [Amazon S3](https://aws.amazon.com/s3/faqs/) or [Glacier](https://aws.amazon.com/s3/glacier/). It is fast, serverless, and works using standard SQL queries.  
    
-   [Amazon Elastic MapReduce (EMR)](https://aws.amazon.com/emr/faqs/) processes big data using servers like Spark and Hadoop.
-    [Amazon Kinesis](https://aws.amazon.com/kinesis/) allows aggregation and processing of streaming data in real-time. It uses website clickstreams, application logs, and telemetry data from IoT devices. 
-   [Amazon OpenSearch](https://aws.amazon.com/opensearch-service/) allows search, analysis, and visualization of petabytes of data.  
    

## What does a data scientist do?

A data scientist can use a range of different techniques, tools, and technologies as part of the data science process. Based on the problem, they pick the best combinations for faster and more accurate results.

A data scientist’s role and day-to-day work vary depending on the size and requirements of the organization. While they typically follow the data science process, the details may vary. In larger data science teams, a data scientist may work with other analysts, engineers, machine learning experts, and statisticians to ensure the data science process is followed end-to-end and business goals are achieved. 

However, in smaller teams, a data scientist may wear several hats. Based on experience, skills, and educational background, they may perform multiple roles or overlapping roles. In this case, their daily responsibilities might include engineering, analysis, and machine learning along with core data science methodologies.   

## What are the challenges faced by data scientists?

### Multiple data sources

Different types of apps and tools generate data in various formats. Data scientists have to clean and prepare data to make it consistent. This can be tedious and time-consuming.

### Understanding the business problem

Data scientists have to work with multiple stakeholders and business managers to define the problem to be solved. This can be challenging—especially in large companies with multiple teams that have varying requirements.

### Elimination of bias

Machine learning tools are not completely accurate, and some uncertainty or bias can exist as a result. Biases are imbalances in the training data or prediction behavior of the model across different groups, such as age or income bracket. For instance, if the tool is trained primarily on data from middle-aged individuals, it may be less accurate when making predictions involving younger and older people. The field of machine learning provides an opportunity to address biases by detecting them and measuring them in the data and model.

---
一条线：数据收集。
一条线：数据分析。

---

