#data-science/tutorial/chapter8  
# 第八章 梯度下降
[为什么梯度方向是函数值增加最快的方向？](https://zhuanlan.zhihu.com/p/38525412)
[常用的梯度下降算法](https://zhuanlan.zhihu.com/p/31630368)
[代码演示](https://machinelearningmastery.com/gradient-descent-with-momentum-from-scratch/)
[Gradient Descent for Machine Learning (ML) 101 with Python Tutorial](https://pub.towardsai.net/gradient-descent-algorithm-for-machine-learning-python-tutorial-ml-9ded189ec556)
## 1. 梯度下降背后的思想
## 2. 梯度的计算
## 3. 使用梯度
## 4. 选择合适的步长
+ 使用固定的步长。
+ 随时间缩小步长。
## 5. 使用梯度下降训练模型
## 6. 小批量和随机梯度下降

------------------------------------------------------------------------------------------------------------

机器学习其实就是找到一个函数，那么怎么找到这个函数呢？往往是通过最小化损失函数的方法来找到要找的函数。这便牵扯到求函数最小值的问题，梯度下降算法是求函数最小值的最常用算法。
## 什么是梯度下降？

## 梯度方向为什么是函数值变化最快的方向？
[梯度方向为什么是函数值变化最快的方向](https://sootlasten.github.io/2017/gradient-steepest-ascent/)

## 梯度下降的公式
$$\theta = \theta - \eta\nabla_\theta J(\theta)$$

在进行更新时，有一点要注意的是，**要同时更新所有的参数**。
公式中有两个关键点，一个是使用多少训练数据来计算梯度，一个是步长的设置。根据计算梯度时使用的训练数据的多少不同，可以分为：梯度下降算法又可以分为批量梯度下降算法（Batch Gradient Descent），随机梯度下降算法（Stochastic Gradient Descent）和小批量梯度下降算法（Mini-batch Gradient Descent）。

根据不同的步长设置策略，可以分为：Momentum、NAG、Adagrad、Adadelta、RMSprop、Adam、AdaMax、Nadam。
## 梯度下降有哪些变种？
### 1.批量梯度下降
### 2.随机梯度下降
### 3.小批量梯度下降
## 梯度下降学习速率的设置
梯度下降算法中一个重要的参数是学习速率，适当的学习速率很重要：学习速率过小时收敛速度慢，而过大时导致训练震荡，而且可能会发散。理想的梯度下降算法要满足两点：收敛速度要快；而且能全局收敛。
### 1.Momentum
$$v_t = \gamma v_{t-1} + \eta\nabla_\theta J(\theta)$$
$$\theta = \theta - v_t$$
### 2.NAG
$$v_t = \gamma v_{t-1} + \eta \nabla_\theta J(\theta - \gamma v_{t-1} )$$
$$\theta = \theta - v_t$$
### 3.Adagrad
### 4.Adadelta
### 5.RMSprop
### 6.Adam
### 7.AdaMax
### 8.Nadam
## 梯度下降面临的挑战
### 1.只能得到局部最优而不是全局最优
梯度下降算法没有办法区分局部最优和全局最优。
![[Pasted image 20220714163859.png]]
![[Pasted image 20220714164219.png]]
### 2.步长大小的选择
一个适中的步长能够使得算法更快地收敛。
![[Pasted image 20220714164916.png]]
如果步长过长，算法有可能永远都不会收敛或者收敛地很慢。
![[Pasted image 20220714165115.png]]
![[Pasted image 20220714165150.png]]
如果步长过小，算法大概率情况下会收敛，但是会很慢。
![[Pasted image 20220714165322.png]]
### 3.梯度下降算法只适用于在每处都可微的函数
![[Pasted image 20220714165457.png]]


```python
# example of plotting a gradient descent search on a one-dimensional function
from numpy import asarray
from numpy import arange
from numpy.random import rand
from numpy.random import seed
from matplotlib import pyplot
 
# objective function
def objective(x):
	return x**2.0
 
# derivative of objective function
def derivative(x):
	return x * 2.0
 
# gradient descent algorithm
def gradient_descent(objective, derivative, bounds, n_iter, step_size):
	# track all solutions
	solutions, scores = list(), list()
	# generate an initial point
	solution = bounds[:, 0] + rand(len(bounds)) * (bounds[:, 1] - bounds[:, 0])
	# run the gradient descent
	for i in range(n_iter):
		# calculate gradient
		gradient = derivative(solution)
		# take a step
		solution = solution - step_size * gradient
		# evaluate candidate point
		solution_eval = objective(solution)
		# store solution
		solutions.append(solution)
		scores.append(solution_eval)
		# report progress
		print('>%d f(%s) = %.5f' % (i, solution, solution_eval))
	return [solutions, scores]
 
# seed the pseudo random number generator
seed(4)
# define range for input
bounds = asarray([[-1.0, 1.0]])
# define the total iterations
n_iter = 30
# define the step size
step_size = 0.1
# perform the gradient descent search
solutions, scores = gradient_descent(objective, derivative, bounds, n_iter, step_size)
# sample input range uniformly at 0.1 increments
inputs = arange(bounds[0,0], bounds[0,1]+0.1, 0.1)
# compute targets
results = objective(inputs)
# create a line plot of input vs result
pyplot.plot(inputs, results)
# plot the solutions found
pyplot.plot(solutions, scores, '.-', color='red')
# show the plot
pyplot.show()
```

```python
# example of plotting gradient descent with momentum for a one-dimensional function
from numpy import asarray
from numpy import arange
from numpy.random import rand
from numpy.random import seed
from matplotlib import pyplot
 
# objective function
def objective(x):
	return x**2.0
 
# derivative of objective function
def derivative(x):
	return x * 2.0
 
# gradient descent algorithm
def gradient_descent(objective, derivative, bounds, n_iter, step_size, momentum):
	# track all solutions
	solutions, scores = list(), list()
	# generate an initial point
	solution = bounds[:, 0] + rand(len(bounds)) * (bounds[:, 1] - bounds[:, 0])
	# keep track of the change
	change = 0.0
	# run the gradient descent
	for i in range(n_iter):
		# calculate gradient
		gradient = derivative(solution)
		# calculate update
		new_change = step_size * gradient + momentum * change
		# take a step
		solution = solution - new_change
		# save the change
		change = new_change
		# evaluate candidate point
		solution_eval = objective(solution)
		# store solution
		solutions.append(solution)
		scores.append(solution_eval)
		# report progress
		print('>%d f(%s) = %.5f' % (i, solution, solution_eval))
	return [solutions, scores]
 
# seed the pseudo random number generator
seed(4)
# define range for input
bounds = asarray([[-1.0, 1.0]])
# define the total iterations
n_iter = 30
# define the step size
step_size = 0.1
# define momentum
momentum = 0.3
# perform the gradient descent search with momentum
solutions, scores = gradient_descent(objective, derivative, bounds, n_iter, step_size, momentum)
# sample input range uniformly at 0.1 increments
inputs = arange(bounds[0,0], bounds[0,1]+0.1, 0.1)
# compute targets
results = objective(inputs)
# create a line plot of input vs result
pyplot.plot(inputs, results)
# plot the solutions found
pyplot.plot(solutions, scores, '.-', color='red')
# show the plot
pyplot.show()
```